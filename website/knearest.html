<!DOCTYPE html>
<html lang="en-us">

<head>
  <link rel="icon" href="resources/heart.png"
  <meta charset="UTF-8">
  <title>ML  & Heart Disease</title>

 <!-- Bring in our bootstrap stylesheet -->

 
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

 <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
 <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
 <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

 <link rel="stylesheet" href="style.css">
</head>

<body>
    <nav class="navbar navbar-expand-lg navbar-dark bg-primary">
      <img src="resources/beat.gif" height="42" width="42">
        <a class="navbar-brand" href="index.html">ML Heart</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarColor01" aria-controls="navbarColor01" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>       
      
        <div class="collapse navbar-collapse" id="navbarColor01">
          <ul class="navbar-nav mr-auto">
            <li class="nav-item active">
              <a class="nav-link" href="index.html">Home</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="etl.html">ETL on Data</a>
              </li>
            <li class="nav-item dropdown active">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  ML Models
                </a>
                <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="forest.html">Random Forest</a>
                  <a class="dropdown-item" href="logistic.html">Logistic Regression </a>
                  <a class="dropdown-item" href="neural.html">Neural Net</a>
                  <a class="dropdown-item active" href="knearest.html">K Nearest Neighbors</a>
                  <a class="dropdown-item" href="vector.html">Support Vector Machine</a>
                </div>
              </li>

            <li class="nav-item">
              <a class="nav-link"href="analysis.html">Analysis</a>
            </li>
          </ul>
          </div>
        </nav>
  <div class="container">


  
      <!-- Row 1 -->
      <div class="row">
        <div class="col-12 col-sm-12 col-md-12">
            <div class="jumbotron">
                 <h1><strong>K-Nearest Neighbor (KNN) Machine Learning Algorithm</strong></h1>
                 <p> 
                  <p><strong>Introduction</strong></p>
<p>A k-nearest-neighbor (KNN) algorithm, is an approach to data classification that estimates how likely a data point is to be a member of one group or the other depending on what group the data points nearest to it are in.</p>
<p>Hereâ€™s the basic idea of how k-NN works: suppose there are two different classes, Class A and Class B, in the dataset. Now we have a new data point, which is the red, pentagon-shaped point on the plot below and we want to predict which class this new data point belongs to. When k = 3, there are 2 Class B and 1 Class A in the 3 nearest neighbors of the new data point. The simple majority vote is Class B. Therefore, we predict the new data point belongs to Class B.</p>
<center> <img src="resources/KNN-img1.png" width = 700 height=500></center>
<p>To generalize, here are the steps of how k-NN works:</p>
<ol>
  <li>Getting the labeled data ready</li>
  <li>Pick an appropriate k</li>
  <li>Get the new sample to classify</li>
  <li>Select the k entries that are closest to the new sample</li>
  <li>Take a simple majority vote to pick a category for new sample</li>
</ol>
<p><u>Two Parameters of k-NN</u></p>
<p>Although the k-NN algorithm is non-parametric, there are two parameters we usually use to build the model:&nbsp;k (the number of neighbors that will vote)and the&nbsp;distance metric.</p>
<p>There are no strict rules around the selection of&nbsp;k. It really depends on the dataset as well as your experience when it comes to choosing an optimal k. When k is small, the prediction would be easily impacted by noise. When k is getting larger, although it reduces the impact of outliers, it will introduce more bias (Think about when we increase k to n, the number of data points in the dataset. The prediction will be the majority class in the dataset).</p>
<p>The selection of the other parameter,&nbsp;distance metric, also varies on different cases. By default, people use Euclidean distance (L2 norm). However, Manhattan distance and Minkowski distance might also be great choices in certain scenarios.</p>
<p><u>Pros and Cons of k-NN</u></p>
<p>There are multiple advantages of using k-NN:</p>
<ol>
  <li>It is a simple machine learning model. Also very easy to implement and interpret.</li>
  <li>There is no training phase of the model.</li>
  <li>There are no prior assumptions on the distribution of the data. This is especially helpful when we have ill-tempered data.</li>
  <li>Believe it or not, k-NN has a relatively high accuracy.</li>
</ol>
<p>Of course, there are disadvantages of the model:</p>
<ol>
  <li>High requirements on memory. We need to store all the data in memory in order to run the model.</li>
  <li>Computationally expensive. Recall that the model works in the way that it selects the k nearest neighbors. This means that we need to compute the distance between the new data point to all the existing data points, which is quite expensive in computation.</li>
  <li>Sensitive to noise. Imagine we pick a really small k, the prediction result will be highly impacted by the noise if there is any.</li>
</ol>
<p>&nbsp;</p>
<p><strong>Implementation on the Heart Disease dataset&nbsp;</strong></p>
<p>We used KNN as a classifier algorithm.&nbsp;&nbsp;After splitting the heart disease data into train and test sets, the data was scaled and encoded. A KNN classifier model was setup, we looped through a list of K values to determine the best accuracy score.</p>
<p>The results showed that the K=5 results in the best score for both train and test data.</p>
                  <p><strong>Implementation on the Heart Disease dataset&nbsp;</strong></p>
                  <p>We used KNN as a classifier algorithm.&nbsp;&nbsp;After splitting the heart disease data into train and test sets, the data was scaled and encoded. A KNN classifier model was setup, we looped through a list of K values to determine the best accuracy score.</p>
                  <p>The results showed that the K=5 results in the best score for both train and test data.</p>
                   <center><img src="resources/k5which.jpg" alt="K nearest results"></center>       
         </p>
         <p>The overall accuracy of the model was 0.784.</p>
         <p><u>Algorithm modification and tuning:&nbsp;</u>&nbsp; Given the limited data samples, K-Fold Cross-Validation was used to resample and evaluate the KNN model. &nbsp;</p>
         <p>In k-fold cross-validation, the original sample is randomly partitioned into k equal size sub samples. Of the k sub samples, a single sub sample is retained as the validation data for testing the model, and the remaining k-1 sub samples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k sub samples used exactly once as the validation data. The k results from the folds can then be averaged (or otherwise combined) to produce a single estimation. The advantage of this method is that all observations are used for both training and validation, and each observation is used for validation exactly once.</p>
         <p>In addition, GridSearchCV along with cross-validation was used to evaluate the model across various parameters. &nbsp;</p>
         <p>The graph below shows mean accuracy across the different k values after the k-fold cross-validation and GridSearchCV was used.</p>
 <center> <img src="resources/accuracy.jpg"></center>
</p>
        </div>
    </div>
      </div>
      </div>
</body>
</html>